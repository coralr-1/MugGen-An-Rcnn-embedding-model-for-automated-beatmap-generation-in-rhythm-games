{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0dba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import librosa\n",
    "from math import ceil\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from fractions import Fraction\n",
    "import pickle\n",
    "from time import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import find_peaks, windows, convolve\n",
    "import shutil\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DIC={\n",
    "    0:[0, 0, 0, 0], 1:[0, 0, 0, 1], 2:[0, 0, 1, 0], 3:[0, 0, 1, 1],\n",
    "    4:[0, 1, 0, 0], 5:[0, 1, 0, 1], 6:[0, 1, 1, 0], 7:[0, 1, 1, 1],\n",
    "    8:[1, 0, 0, 0], 9:[1, 0, 0, 1], 10:[1, 0, 1, 0], 11:[1, 0, 1, 1],\n",
    "    12:[1, 1, 0, 0], 13:[1, 1, 0, 1], 14:[1, 1, 1, 0], 15:[1, 1, 1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb9bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建模型类：MugGenModel_T判断音频的时间步上是否出现音符\n",
    "\n",
    "\n",
    "class MugGenModel_T(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MugGenModel_T, self).__init__()\n",
    "        self.train_loss=[0]\n",
    "        self.test_loss=[0]\n",
    "        self.train_F1=[]\n",
    "        self.test_F1=[]\n",
    "        self.train_Recall=[]\n",
    "        self.test_Recall=[]\n",
    "        self.train_Precision=[]\n",
    "        self.test_Precision=[]\n",
    "\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(5,3))\n",
    "        self.pool0 = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3))\n",
    "        self.conv1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5,3))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=120, kernel_size=(5,1))\n",
    "        self.fc0 = nn.Linear(960, 512)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, Input):\n",
    "        batch_size,_,seq_len,_ = Input.size()\n",
    "\n",
    "        '''\n",
    "        IN SHAPE:(BATCH,CHANNEL,SEQ,FEATURES)\n",
    "        IN(BATCH,3,50,80)->CONV0->(BATCH,10,25,78)-POOL0->(BATCH,10,1,26)->CONV1->(BATCH,20,1,24)->POOL1->(BATCH,20,1,8)\n",
    "        TRANSPOSE & RESIZE->(BATCH,1,20*8)->LINEAR0(BATCH_SIZE,1,256)->LINEAR1(BATCH_SIZE,1,128)->LINEAR2(BATCH_SIZE,1)\n",
    "        ->OUT(BATCH_SIZE,2)\n",
    "        '''\n",
    "        conv_out0 = self.conv0(Input)\n",
    "        conv_out0 = self.dropout(self.pool0(self.relu(conv_out0)))\n",
    "        \n",
    "        conv_out1 = self.conv1(conv_out0)\n",
    "        conv_out1 = self.dropout(self.pool1(self.relu(conv_out1)))\n",
    "        \n",
    "        conv_out1 = self.relu(self.conv2(conv_out1))\n",
    "\n",
    "        conv_out1 = conv_out1.transpose(1,2).contiguous()\n",
    "        \n",
    "        conv_out1 = conv_out1.reshape(batch_size,1,-1)\n",
    "        \n",
    "        \n",
    "        liner_out0=self.fc0(conv_out1)\n",
    "        liner_out1=self.fc1(liner_out0)\n",
    "        output = self.fc2(liner_out1)\n",
    "        \n",
    "\n",
    "        return output.flatten()#,hidden#torch.sigmoid()\n",
    "    \n",
    "\n",
    "    \n",
    "## 构建模型类：MugGenModel_N判断音频的时间步上音符的排列形状\n",
    "class MugGenModel_N(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MugGenModel_N, self).__init__()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.embed=nn.Embedding(16,4)\n",
    "        self.embedfc0=nn.Linear(4*16+8,128)\n",
    "        self.embedfc1=nn.Linear(128,64)\n",
    "        \n",
    "        self.conv=nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3),padding=(1,1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2),padding=(1,0))\n",
    "        self.conv1=nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3),padding=(1,1))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2),padding=(1,0))\n",
    "        self.conv2=nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3),padding=(1,1))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2),padding=(1,0))\n",
    "        self.conv3=nn.Conv2d(in_channels=64, out_channels=8, kernel_size=(3,3),padding=(1,1))\n",
    "        self.fc0=nn.Linear(30*8+8,512)\n",
    "        self.fc1=nn.Linear(512,128)\n",
    "        \n",
    "        self.outfc0=nn.Linear(128+64,256)\n",
    "        self.outfc1=nn.Linear(256,16)\n",
    "        \n",
    "    def forward(self, Input,Last_note,note_distinct1,note_distinct2):\n",
    "        conv_out=self.pool(self.relu(self.conv(Input)))\n",
    "        conv_out=self.pool1(self.relu(self.conv1(conv_out)))\n",
    "        conv_out=self.pool2(self.relu(self.conv2(conv_out)))\n",
    "        conv_out=self.relu(self.conv3(conv_out))\n",
    "        conv_out=conv_out.squeeze().reshape(conv_out.shape[0],-1)\n",
    "        conv_out=torch.cat((note_distinct1,note_distinct2,conv_out),dim=-1)\n",
    "        conv_out=self.relu(self.fc0(conv_out))\n",
    "        conv_out=self.dropout(conv_out)\n",
    "        conv_out=self.fc1(conv_out)\n",
    "        \n",
    "        ebd_out=self.embed(Last_note)\n",
    "        ebd_out=torch.cat((note_distinct1,note_distinct2,ebd_out.reshape(ebd_out.shape[0],-1)),dim=-1)\n",
    "        ebd_out=self.dropout(self.relu(self.embedfc0(ebd_out)))\n",
    "        ebd_out=self.embedfc1(ebd_out)\n",
    "        \n",
    "        out=torch.cat((conv_out,ebd_out),dim=-1)\n",
    "        out=self.relu(self.outfc0(out))\n",
    "        out=self.outfc1(out)\n",
    "        return out\n",
    "    \n",
    "class LSTM_T(nn.Module):\n",
    "    def __init__(self):\n",
    "        global DIC,MEL_DIM,HIDDEN_DIM\n",
    "        super(LSTM_T, self).__init__()\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        self.lstm = nn.LSTM(240,200, num_layers=2,dropout=0.3,bidirectional=True,batch_first=True)\n",
    "        self.fc0=nn.Linear(400,256)\n",
    "        self.fc1=nn.Linear(256,128)\n",
    "        self.fc2=nn.Linear(128,1)\n",
    "        \n",
    "    def forward(self,Input,hidden=None):\n",
    "        batch_size=Input.shape[0]\n",
    "        if hidden==None:\n",
    "            lstmout,hidden=self.lstm(Input.transpose(1,2).reshape(batch_size,-1,240))\n",
    "        else:\n",
    "            lstmout,hidden=self.lstm(Input.transpose(1,2).reshape(batch_size,-1,240),hidden)\n",
    "        #print(lstmout.shape)\n",
    "        out=self.relu(self.fc0(lstmout))\n",
    "        #out=self.relu(self.fc0(lstmout))\n",
    "        out=self.relu(self.fc1(out))\n",
    "        out=self.fc2(out)\n",
    "        return out,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1859c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def music_mel(y,sr,step=10,mel_dim=80):#默认step是10毫秒\n",
    "    hop_length = ceil(sr / (22050/220))  # 计算hop_length\n",
    "    # 提取音频特征\n",
    "    spectrogram1 = librosa.feature.melspectrogram(y=y, sr=sr,n_mels=mel_dim,hop_length=hop_length,n_fft=512)\n",
    "    spectrogram2 = librosa.feature.melspectrogram(y=y, sr=sr,n_mels=mel_dim,hop_length=hop_length,n_fft=1024)\n",
    "    spectrogram3 = librosa.feature.melspectrogram(y=y, sr=sr,n_mels=mel_dim,hop_length=hop_length,n_fft=2048)\n",
    "    log_spectrogram1 = Normalization(librosa.power_to_db(spectrogram1))\n",
    "    log_spectrogram2 = Normalization(librosa.power_to_db(spectrogram1))\n",
    "    log_spectrogram3 = Normalization(librosa.power_to_db(spectrogram1))\n",
    "    return np.stack((log_spectrogram1,log_spectrogram2,log_spectrogram3),axis=2)\n",
    "def Normalization(datas):\n",
    "    Min = np.min(datas)\n",
    "    Max = np.max(datas)\n",
    "    return (datas-Min)/(Max-Min)\n",
    "\n",
    "\n",
    "\n",
    "def get_frame_labels(inputs,peaks,seq_len=5):\n",
    "    #将输入的音乐转化为音乐片段，只选取有音符部分的音乐和标签\n",
    "    #返回当前note与上一个note的距离,分为4个类别，32~16分，16~8分，8~4分和4分以上的音符\n",
    "    res=[]\n",
    "    res_l=[]\n",
    "    inupts=np.pad(inputs,((0,0),(seq_len,seq_len+1),(0,0)))\n",
    "    distincts=[]\n",
    "    di=0\n",
    "    for i in range(seq_len,inputs.shape[1]-seq_len-1):\n",
    "        if i in peaks:\n",
    "            if i-di<=5:#约50毫秒以内，通常是32分音符\n",
    "                di_type=[1,0,0,0]\n",
    "            elif i-di<=10:#约100毫秒以内，通常是16分音符\n",
    "                di_type=[0,1,0,0]\n",
    "            elif i-di<=20:#约200毫秒以内，通常是8分音符\n",
    "                di_type=[0,0,1,0]\n",
    "            else:#其他距离视作同一种类别的音符\n",
    "                di_type=[0,0,0,1]\n",
    "            distincts.append(di_type)\n",
    "            di=i\n",
    "            res.append(inputs[:,i-seq_len:i+seq_len+1,:])\n",
    "    res=np.array(res)\n",
    "    distincts1=np.array(distincts)\n",
    "    distincts.append([0,0,0,1])\n",
    "    distincts=distincts[1:]\n",
    "    distincts2=np.array(distincts)\n",
    "    return res,distincts1,distincts2\n",
    "\n",
    "\n",
    "def tensor2mc(data,music_name,steptime=10,bpm=120,offset=0):\n",
    "    global DIC\n",
    "    res_data={\"meta\":{\"$ver\":0,\n",
    "                      \"creator\":\"MugGenModel\",\n",
    "                      \"background\":\"\",\n",
    "                      \"version\":\"\",\n",
    "                      \"id\":0,\n",
    "                      \"mode\":0,\n",
    "                      \"time\":0,\n",
    "                      \"song\":{\"title\":f\"{music_name}\",\"artist\":\"\",\"id\":0},\n",
    "                      \"mode_ext\":{\"column\":4,\"bar_begin\":0}\n",
    "                     },\n",
    "               \"time\":[{\"beat\":[0,0,1],\"bpm\":bpm}],\n",
    "               \"effect\":[],\n",
    "               \"note\":[{\"beat\":[0,0,1],\"sound\":music_name,\"vol\":100,\"offset\":offset,\"type\":1}],\n",
    "               \"extra\":{\"test\":{\"divide\":4,\"speed\":100,\"save\":0,\"lock\":0,\"edit_mode\":0}}}\n",
    "    \n",
    "\n",
    "    notes=[]\n",
    "    for i in data:\n",
    "        notes.append(DIC[i.item()])\n",
    "        \n",
    "    note=[]\n",
    "    for i in range(len(notes)):\n",
    "        for col in range(len(notes[i])):\n",
    "            if notes[i][col]==1:\n",
    "                b0=int((i*steptime+50-offset)/1000/60*bpm)\n",
    "                fraction_result = Fraction((i*steptime+50-offset)/1000/60*bpm-b0).limit_denominator()\n",
    "                b1,b2=fraction_result.numerator, fraction_result.denominator\n",
    "                original_fraction = Fraction(b1, b2) # 原始分数\n",
    "                target_denominator = 128 # 目标分母\n",
    "                # 方法是先将分数转换为小数，然后乘以目标分母，并四舍五入得到新的分子\n",
    "                new_numerator = round(original_fraction * target_denominator)\n",
    "               # 创建新的分数\n",
    "                converted_fraction = Fraction(new_numerator, target_denominator)\n",
    "                b1,b2=converted_fraction.numerator, converted_fraction.denominator\n",
    "                note.append({'beat':[b0,b1,b2],'column':col})\n",
    "    res_data['note']=note+res_data['note']\n",
    "    return res_data\n",
    "\n",
    "def get_Bpm(peaks):\n",
    "    #获取一个大概的bpm和offset，由于使用了（220/22050）ms作为一个时间步，所以并不是很准确\n",
    "    dis=[]\n",
    "    dis_dic={}\n",
    "    for i in range(len(peaks)):#寻找相同间隔最多的一类，通常是16分音符\n",
    "        if len(dis)==0:\n",
    "            dis.append(peaks[i])\n",
    "        else:\n",
    "            dis.append(peaks[i]-peaks[i-1])\n",
    "    for i in dis:\n",
    "        if i not in dis_dic.keys():\n",
    "            dis_dic[i]=1\n",
    "        else:\n",
    "            dis_dic[i]+=1\n",
    "    dis_dic=sorted(dis_dic.items(),key=lambda x:x[1])\n",
    "    i=-1\n",
    "    time=(dis_dic[-1][0]*dis_dic[-1][1]+dis_dic[-2][0]*dis_dic[-2][1])/(dis_dic[-1][1]+dis_dic[-2][1])\n",
    "    bpm=60/(220/22050*time*4) #16分音符的间隔*4就是一拍的时间\n",
    "    #return bpm\n",
    "    while bpm<150 or bpm>300:#缩放至150-300bpm之间\n",
    "        if bpm<150:\n",
    "            bpm*=2\n",
    "        if bpm>300:\n",
    "            bpm/=2\n",
    "    bpm=round(bpm,3)\n",
    "    \n",
    "    offset_=0\n",
    "    for i in dis:\n",
    "        if i!=time:\n",
    "            offset_+=i\n",
    "        else:\n",
    "            break\n",
    "    offset=(offset_%time)*(220/22050)*1000\n",
    "    return bpm,offset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8dd8c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T=MugGenModel_T().to(device)\n",
    "N=MugGenModel_N().to(device)\n",
    "checkpoint = torch.load('model/T/model .pth')\n",
    "T.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint = torch.load('model/N/model.pth')\n",
    "N.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b099da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N2 包含前后note距离信息\n",
    "music_path='music/'\n",
    "music_name='1684656672.ogg'\n",
    "music_bpm=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f3dd41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818c55677f784727b92c81b8d846d541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Mug_Gen_T预测音符\n",
    "threshold0=0\n",
    "threshold1=0.8\n",
    "\n",
    "test_music_temp=[]\n",
    "res=[]\n",
    "y,sr=librosa.load(music_path+music_name)\n",
    "in_data=music_mel(y,sr,mel_dim=80).transpose(2,1,0)\n",
    "for k in tqdm(range(6,in_data.shape[1]-7)):\n",
    "    test_music_temp.append(in_data[:,k-6:(k+7),:])\n",
    "    if len(test_music_temp)>=16 or k>=in_data.shape[1]-128:\n",
    "        with torch.no_grad():\n",
    "            test_music_temp=torch.tensor(np.array(test_music_temp), dtype=torch.float32).to(device)\n",
    "            output = T(test_music_temp)\n",
    "            res+=(output.cpu().tolist())\n",
    "            test_music_temp=[]\n",
    "sig_res=torch.sigmoid(torch.tensor(res)).numpy()\n",
    "window = windows.hamming(5)\n",
    "smoothed_pred = convolve(sig_res, window / window.sum(), mode='same')\n",
    "peaks,_=find_peaks(smoothed_pred, height=threshold0*sum(sig_res)/len(sig_res),distance=3,prominence=threshold1*sum(sig_res)/len(sig_res))\n",
    "bpm,offset=get_Bpm(peaks)\n",
    "a,b,c=get_frame_labels(in_data,peaks+14,seq_len=8)\n",
    "offset=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ad55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77a8cf07cb3429bbe161b7498c36b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qewqer\\AppData\\Local\\Temp\\ipykernel_33152\\699383603.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  out_temp=N(torch.tensor(np.array([a[i]])).to(device),last_note,torch.tensor([b[i]]).to(device),torch.tensor([c[i]]).to(device))\n"
     ]
    }
   ],
   "source": [
    "last_note=torch.zeros((1,16),dtype=torch.long).to(device)\n",
    "out=None\n",
    "for i in tqdm(range(a.shape[0])):\n",
    "    out_temp=N(torch.tensor(np.array([a[i]])).to(device),last_note,torch.tensor([b[i]]).to(device),torch.tensor([c[i]]).to(device))\n",
    "    cur_note=torch.argmax(out_temp,dim=1)[None,:]\n",
    "    last_note=torch.cat((cur_note,last_note),dim=1)[:,:-1]\n",
    "    if type(out)==type(None):\n",
    "        out=cur_note\n",
    "    else:\n",
    "        out=torch.cat((out,cur_note),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd263f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'export/1714831503 1684656672.ogg\\\\1684656672.ogg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=torch.zeros(in_data.shape[1])\n",
    "for i in range(len(peaks)):\n",
    "    try:\n",
    "        res[peaks[i]]=out[0][i]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "mc=tensor2mc(res,music_name,steptime=1000/(22050/220),bpm=bpm,offset=offset)\n",
    "\n",
    "dir_name=f\"export/{int(time())} \"+music_name\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "filename = dir_name+\"/0.mc\"\n",
    "with open(filename, \"w\") as file:\n",
    "    file.write(str(mc).replace('\\'','\"'))\n",
    "    \n",
    "\n",
    "# 使用shutil.move()函数移动文件\n",
    "shutil.copy(music_path+music_name, dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d590fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34fa19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
